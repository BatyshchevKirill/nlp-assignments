{
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 7879091,
     "sourceType": "datasetVersion",
     "datasetId": 4598785
    },
    {
     "sourceId": 7917417,
     "sourceType": "datasetVersion",
     "datasetId": 4652266
    },
    {
     "sourceId": 3167333,
     "sourceType": "datasetVersion",
     "datasetId": 1887500
    }
   ],
   "dockerImageVersionId": 30673,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ],
   "metadata": {
    "id": "DIgM6C9HYUhm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "You may also want to implement:\n",
    "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
    "- some recent (or not very recent) paper on this topic,\n",
    "- solution which takes into account keyboard layout and associated misspellings,\n",
    "- efficiency improvement to make the solution faster,\n",
    "- any other idea of yours to improve the Norvigâ€™s solution.\n",
    "\n",
    "IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ],
   "metadata": {
    "id": "x-vb8yFOGRDF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the original Norvig's solution. It will be used for performance comparison. Its workflow is simple: compute all words that are at distance 1-2 from the one to correct, and then choose the one which has the biggest occurence probability"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "w = words(open('/kaggle/input/ass2base/big.txt').read())\n",
    "WORDS = Counter(w)\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T02:55:07.330039Z",
     "iopub.execute_input": "2024-03-23T02:55:07.330416Z",
     "iopub.status.idle": "2024-03-23T02:55:08.001768Z",
     "shell.execute_reply.started": "2024-03-23T02:55:07.330388Z",
     "shell.execute_reply": "2024-03-23T02:55:08.000650Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is my solution. It is based on Norvig's idea, but improved to handle the context using Ngrams. The idea behind it is the following:<br>\n",
    "1. The text is tokenized, only tokens that are pure alphabetic words are chosen to participate in correction\n",
    "1. For each word, we correct it if it is not in our dictionary or is a short but not often word\n",
    "1. We compute the candidates for the word as in Norvig's solution (the word itself and 1-2 distance words)\n",
    "1. All ngram probabilities are computed. If ngrams param is 3, for example, and we want to correct the middle word in the text \"a a w a a\", then all ngrams will be [\"w\", \"a w\", \"w a\", \"a a w\", \"a w a\", \"w a a\"]. The probability of ngram \"a a w\", for example, is counts(\"a a w\") / sum(counts(\"a a \\*\")) where \\* is any word. The probabilities were computed in the preprocessing stage which is not included in the notebook, but will be described below.\n",
    "1. If a word has the highest probability out of all candidates, it becomes the correction.\n",
    "1. Finally, the text is postprocessed to return to the initial shape\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the ngram dictionaries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open(\"/kaggle/input/bigrams-jsons/bigrams_key_word1.json\", \"r\") as f:\n",
    "    dct1 = json.load(f)\n",
    "with open(\"/kaggle/input/bigrams-jsons/bigrams_key_word2.json\", \"r\") as f:\n",
    "    dct2 = json.load(f)\n",
    "with open(\"/kaggle/input/bigrams-jsons/trigrams_key_word1_word2.json\", \"r\") as f:\n",
    "    dct3 = json.load(f)\n",
    "with open(\"/kaggle/input/bigrams-jsons/trigrams_key_word1_word3.json\", \"r\") as f:\n",
    "    dct4 = json.load(f)\n",
    "with open(\"/kaggle/input/bigrams-jsons/trigrams_key_word2_word3.json\", \"r\") as f:\n",
    "    dct5 = json.load(f)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T02:59:47.138678Z",
     "iopub.execute_input": "2024-03-23T02:59:47.139086Z",
     "iopub.status.idle": "2024-03-23T02:59:47.144786Z",
     "shell.execute_reply.started": "2024-03-23T02:59:47.139054Z",
     "shell.execute_reply": "2024-03-23T02:59:47.143331Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dcts = [dct1, dct2, dct3, dct4, dct5]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T02:57:54.638947Z",
     "iopub.execute_input": "2024-03-23T02:57:54.639326Z",
     "iopub.status.idle": "2024-03-23T02:57:54.644695Z",
     "shell.execute_reply.started": "2024-03-23T02:57:54.639298Z",
     "shell.execute_reply": "2024-03-23T02:57:54.643427Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implementation itself"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Corrector:\n",
    "    \"\"\"\n",
    "    Context sensitive ngram spell corrector. Main method for interacting is \"correct\".\n",
    "    \"\"\"\n",
    "    def __init__(self, dicts, word_set, unigram_count, ngrams=2):\n",
    "        \"\"\"\n",
    "        The initializer of the class\n",
    "        \n",
    "        Params:\n",
    "            dicts: the list of dictionaries to be used. For each n-gram there should be n dicts. Sorted by ascending n.\n",
    "            word_set: the set of words in our language\n",
    "            unigram_count: total number of words in the corpus used for unigrams (normalizing factor)\n",
    "            ngrams: the maximal n for ngrams\n",
    "        \"\"\"\n",
    "        self.dcts = dicts\n",
    "        self.words = word_set\n",
    "        self.first_norm = unigram_count\n",
    "        self.max_word = None\n",
    "        self.max_prob = 0\n",
    "        self.ngrams=ngrams\n",
    "        \n",
    "    def edits1(word):\n",
    "        \"\"\"\n",
    "        All edits that are one edit away from `word`. From Norvig's implementation. static method\n",
    "        \"\"\"\n",
    "        letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edits2(word): \n",
    "        \"\"\"\n",
    "        All edits that are two edits away from `word`. From Norvig's implementation. static method\n",
    "        \"\"\"\n",
    "        return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "    def best_fit(self, text, func, penalty_scale=1):\n",
    "        \"\"\"\n",
    "        Finds the candidate with the highest probability\n",
    "        \n",
    "        Params:\n",
    "            text: the tokens of the part of text. Middle element is the target for correction\n",
    "            func: the function to generate candidates\n",
    "            penalty_scale: exponential penalty (for example, for far apart words)\n",
    "        \"\"\"\n",
    "        # The index of the correction target\n",
    "        index = len(text) // 2\n",
    "        \n",
    "        for e in func(text[index]):\n",
    "            # The initial unigram probability\n",
    "            prob = (self.words[e] + 1) / self.first_norm\n",
    "\n",
    "            k = 0\n",
    "            for i in range(2, self.ngrams+1):\n",
    "                # For all ngram sizes\n",
    "                \n",
    "                # Generate all i-grams with the candidate in the text\n",
    "                keys = []\n",
    "                for j in range(i + 1):\n",
    "                    ind = self.ngrams - 2 + j\n",
    "                    if ind != index:\n",
    "                        key = text[ind: index] + [e] + text[index + 1:ind+self.ngrams-1]\n",
    "                        key = \" \".join(key)\n",
    "                        keys.append(key)\n",
    "                \n",
    "                # Search for the key in the corresponding dict\n",
    "                for key in keys:\n",
    "                    if key in self.dcts[k]:\n",
    "                        val = self.dcts[k][key]\n",
    "                        val *= len(self.words) ** (i - 2)\n",
    "                        prob *= val\n",
    "                    else:\n",
    "                        # Avoiding zeros\n",
    "                        prob *= 1e-10\n",
    "                    k += 1   \n",
    "            \n",
    "            # Penalize if needed\n",
    "            prob **= penalty_scale\n",
    "            \n",
    "            # Choose the best candidate\n",
    "            if prob > self.max_prob:\n",
    "                self.max_prob = prob\n",
    "                self.max_word = e\n",
    "\n",
    "\n",
    "    def fix(self, text):\n",
    "        \"\"\"\n",
    "        Fix a single word (in the middle of the text)\n",
    "        \n",
    "        Params:\n",
    "            text: the list of tokens. The middle one will be corrected\n",
    "        Returns:\n",
    "            the corrected text piece\n",
    "        \"\"\"\n",
    "        self.max_prob = 0\n",
    "        self.max_word = text[len(text) //2]\n",
    "\n",
    "        self.best_fit(text, lambda x: [x])\n",
    "        self.best_fit(text, Corrector.edits1)\n",
    "        self.best_fit(text, Corrector.edits2, 1.01)\n",
    "\n",
    "        text[1] = self.max_word\n",
    "        return text\n",
    "\n",
    "    def fix_sentence(self, text):\n",
    "        \"\"\"\n",
    "        Fix all words in the given list of tokens.\n",
    "        \n",
    "        Params:\n",
    "            text: list of tokens to fix\n",
    "        Return:\n",
    "            the fixed text\n",
    "        \"\"\"\n",
    "        # Add paddings\n",
    "        text = ['']*(self.ngrams-1) + text + ['']*(self.ngrams-1)\n",
    "        \n",
    "        # Correct all words one by one if needed\n",
    "        for i in range(len(text) - 2):\n",
    "            target = text[i+1]\n",
    "            if target not in self.words or (len(target) < 4 and self.words[target] < 100):\n",
    "                text[i: i+3] = self.fix(text[i: i+3])\n",
    "                \n",
    "        return text[self.ngrams-1: -self.ngrams+1]\n",
    "    \n",
    "    def correct(self, corpus, verbose=1):\n",
    "        \"\"\"\n",
    "        The main method for interacting with the class. Is used to fix the whole corpus of the text\n",
    "        \n",
    "        Params:\n",
    "            corpus: the text corpus\n",
    "            verbose: 1 if display tqdm, 0 otherwise\n",
    "        Returns:\n",
    "            the fixed corpus\n",
    "        \"\"\"\n",
    "        texts = sent_tokenize(corpus)\n",
    "        corrected = []\n",
    "        for text in (tqdm(texts) if verbose else texts):\n",
    "            corrected.append(self.correct_one(text))\n",
    "        return \"\\n\".join(corrected)\n",
    "    \n",
    "    def correct_one(self, text):\n",
    "        \"\"\"\n",
    "        Correct a single sentence. Mainlu used for preprocessing and postprocessing of the text\n",
    "        \"\"\"\n",
    "        # Tokenize the words\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Take only the words that can actively participate in correction (not punctuation or numbers)\n",
    "        pattern = r'^[a-zA-Z]+(\\-[a-zA-Z]+)?\\.?$'\n",
    "        useful = [1 if re.match(pattern, t) is not None else 0 for t in tokens]\n",
    "        word_toks = [t.lower() for i, t in zip(useful, tokens) if i]\n",
    "        \n",
    "        # Fix the sentence\n",
    "        word_toks = self.fix_sentence(word_toks)\n",
    "        \n",
    "        # Replace the corrected words. Pretify the case of the text\n",
    "        j = 0\n",
    "        for i, rep in enumerate(useful):\n",
    "            if rep:\n",
    "                word = word_toks.pop(0)\n",
    "                if tokens[i][0].isupper():\n",
    "                    word = word[0].upper() + word[1:]\n",
    "                if tokens[i].isupper():\n",
    "                    word = word.upper()\n",
    "                tokens[i] = word\n",
    "        \n",
    "        # Pretify punctuation\n",
    "        text = \" \".join(tokens)\n",
    "        text = re.sub(r'\\s+([^\\w\\s])', r'\\1', text)\n",
    "        return text.strip()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T02:57:59.796800Z",
     "iopub.execute_input": "2024-03-23T02:57:59.797233Z",
     "iopub.status.idle": "2024-03-23T02:58:02.210607Z",
     "shell.execute_reply.started": "2024-03-23T02:57:59.797200Z",
     "shell.execute_reply": "2024-03-23T02:58:02.209176Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "c = Corrector(dcts, WORDS, len(w), ngrams=3)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T02:58:07.509725Z",
     "iopub.execute_input": "2024-03-23T02:58:07.510345Z",
     "iopub.status.idle": "2024-03-23T02:58:07.516788Z",
     "shell.execute_reply.started": "2024-03-23T02:58:07.510306Z",
     "shell.execute_reply": "2024-03-23T02:58:07.515294Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Several examples to demonstrate the work of the class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "string = \"I ws boorn on NOVEMBRE the 8th\"\n",
    "my = c.correct(string)\n",
    "norvig = \" \".join(correction(s) for s in string.split())\n",
    "print(f\"Original string: {string}\\nCorrect: {'I was born on NOVEMBER the 8th'}\\nMy correction: {my}\\nNorvig's correction: {norvig}\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T03:34:45.756245Z",
     "iopub.execute_input": "2024-03-23T03:34:45.756627Z",
     "iopub.status.idle": "2024-03-23T03:34:48.228766Z",
     "shell.execute_reply.started": "2024-03-23T03:34:45.756599Z",
     "shell.execute_reply": "2024-03-23T03:34:48.227614Z"
    },
    "trusted": true
   },
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.32s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Original string: I ws boorn on NOVEMBRE the 8th\nCorrect:I was born on NOVEMBER the 8th\nMy correction: I was born on NOVEMBER the 8th\nNorvig's correction: a was born on NOVEMBRE the 8th\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "string = \"Chemical agents used during LrotWst at port ahgusra prixoj. Go between bridye to lprn July 5\"\n",
    "correct = \"Chemical agents used during protest at port augusta prison. Go between bridge to open July 5\"\n",
    "my = c.correct(string)\n",
    "norvig = \" \".join(correction(s) for s in string.split())\n",
    "print(f\"Original string: {string}\\nCorrect: {correct}\\nMy correction: {my}\\nNorvig's correction: {norvig}\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T03:35:57.523149Z",
     "iopub.execute_input": "2024-03-23T03:35:57.523583Z",
     "iopub.status.idle": "2024-03-23T03:36:02.539670Z",
     "shell.execute_reply.started": "2024-03-23T03:35:57.523546Z",
     "shell.execute_reply": "2024-03-23T03:36:02.538383Z"
    },
    "trusted": true
   },
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.32s/it]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Original string: Chemical agents used during LrotWst at port ahgusra prixoj. Go between bridye to lprn July 5\nCorrect: Chemical agents used during protest at port augusta prison. Go between bridge to open July 5\nMy correction: Chemical agents used during Protest at port augusta prison.\nGo between bridge to open July 5\nNorvig's correction: chemical agents used during protest at port augusta prixoj. to between bridge to upon july 5\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "string = \"Go fuk yourself\"\n",
    "correct = \"Go for yourself\"\n",
    "my = c.correct(string)\n",
    "norvig = \" \".join(correction(s) for s in string.split())\n",
    "print(f\"Original string: {string}\\nCorrect: {correct}\\nMy correction: {my}\\nNorvig's correction: {norvig}\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T03:38:03.440804Z",
     "iopub.execute_input": "2024-03-23T03:38:03.441222Z",
     "iopub.status.idle": "2024-03-23T03:38:03.742059Z",
     "shell.execute_reply.started": "2024-03-23T03:38:03.441190Z",
     "shell.execute_reply": "2024-03-23T03:38:03.740633Z"
    },
    "trusted": true
   },
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.46it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Original string: Go fuk yourself\nCorrect: Go for yourself\nMy correction: Go for yourself\nNorvig's correction: to fur yourself\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Justify your decisions\n",
    "\n",
    "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
    "- Which ngram dataset to use\n",
    "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
    "- Beam search parameters\n",
    "- etc."
   ],
   "metadata": {
    "id": "oML-5sJwGRLE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I have made several choices in my implementation:\n",
    "* Use google ngrams for bigrams and COCA for trigrams. The choice was made as the google dataset is very wide and covers a lot of real information, thus displaying the reality in a truthful manner. However, to preprocess the data I had to work with several terabytes of data (to recieve ~1GB), and I couldn't afford to work with the trigram data. So, I have chosen a more simple dataset for its availability.\n",
    "* I have chosen not to correct words that contain digits or punctuation (except for . and -). This is because such misspells are extremely rare, however there are many namings containing the numbers such as \"T4\". Such words and punctuation were left unchanged in their places and did not participate in the correction process. The datasets were preprocessed in the corresponding manner\n",
    "* I decided to add an exponential penalty to the words of distance 2. However, in my test set most of the words contained 2 misspels, and my model showed approximately the same results as Norvig's. Therefore, it was decided to lower the penalty, until it almost vanished.\n",
    "* The maximal number of ngrams was chosen as 3 due to hardware limitations\n",
    "* The participation of ngrams in the probability increases as the number n increases. I just multiply all of them, but there is 1 unigram, 2 bigrams, 3 trigrams, therefore each next layer has a larger impact.\n",
    "* I decided to keep the original style of the text (punctuation, uppercase letters) to keep the text consistent with the original\n",
    "* I decided to use Laplace smoothing on unigrams to avoid zeros, and for other bigrams I just added an insignificant constant, as using Laplace smoothing may be extremely inefficient for large n (basically starting from n >= 2)"
   ],
   "metadata": {
    "id": "6Xb_twOmVsC6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
   ],
   "metadata": {
    "id": "46rk65S4GRSe"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us test the implementation. The test set is a subset of the [dataset](https://www.kaggle.com/datasets/samarthagarwal23/spelling-mistake-data-1mn), but cleaned from mistakes such as number in the middle of the word(nobody makes such mistakes, and it was decided to consider them as correct above). I used only 1000 samples and counted the accuracy of misspeled words' correction. Here are the results:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/spelling-mistake-data-1mn/test.csv\")\n",
    "df = df[~df['augmented_text'].str.contains(r'\\b\\w*\\d+\\w*\\b')]\n",
    "df = df[df['augmented_text'].str.replace(\" \", \"\").str.isalnum()]\n",
    "df"
   ],
   "metadata": {
    "id": "OwZWaX9VVs7B",
    "execution": {
     "iopub.status.busy": "2024-03-23T03:08:27.115727Z",
     "iopub.execute_input": "2024-03-23T03:08:27.116152Z",
     "iopub.status.idle": "2024-03-23T03:08:29.472172Z",
     "shell.execute_reply.started": "2024-03-23T03:08:27.116120Z",
     "shell.execute_reply": "2024-03-23T03:08:29.470980Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": [
    {
     "execution_count": 18,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                     text  \\\n0           project looks to mulesing genetic alternative   \n1       chemical agents used during protest at port au...   \n2       business chamber seeks budget infrastructure b...   \n7       adelaide fashion festival bucks trend on recyc...   \n8            scott pruitt questioned on climate change in   \n...                                                   ...   \n102479    attorney general welcomes terrorism convictions   \n102480       rebels suspected as nepali politician killed   \n102481        lebanon lodges un protest over israeli raid   \n102482             sydney couple accused of tabcorp fraud   \n102484    doctors optimistic after heart surgery for east   \n\n                                           augmented_text  \n0           project looks to muelsnig ngeetic alternative  \n1       chemical agents used during LrotWst at port ah...  \n2       business hcmaber seeks budget infrastrcutuer b...  \n7       adelaide fsahoin festaivl bucks trend on recyc...  \n8            scott pruitt quectikned on cPimzte cNZnge in  \n...                                                   ...  \n102479    attorney TRneral welcomes terrorism convictJohs  \n102480       rebels usspceted as nepali politician killed  \n102481        leabnno lodges un rptoest over irseali raid  \n102482             ysdeny couple accused of tabcorp fraud  \n102484    doctsor optimistic after ehrat surgery for aets  \n\n[57208 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>augmented_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>project looks to mulesing genetic alternative</td>\n      <td>project looks to muelsnig ngeetic alternative</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>chemical agents used during protest at port au...</td>\n      <td>chemical agents used during LrotWst at port ah...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>business chamber seeks budget infrastructure b...</td>\n      <td>business hcmaber seeks budget infrastrcutuer b...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>adelaide fashion festival bucks trend on recyc...</td>\n      <td>adelaide fsahoin festaivl bucks trend on recyc...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>scott pruitt questioned on climate change in</td>\n      <td>scott pruitt quectikned on cPimzte cNZnge in</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>102479</th>\n      <td>attorney general welcomes terrorism convictions</td>\n      <td>attorney TRneral welcomes terrorism convictJohs</td>\n    </tr>\n    <tr>\n      <th>102480</th>\n      <td>rebels suspected as nepali politician killed</td>\n      <td>rebels usspceted as nepali politician killed</td>\n    </tr>\n    <tr>\n      <th>102481</th>\n      <td>lebanon lodges un protest over israeli raid</td>\n      <td>leabnno lodges un rptoest over irseali raid</td>\n    </tr>\n    <tr>\n      <th>102482</th>\n      <td>sydney couple accused of tabcorp fraud</td>\n      <td>ysdeny couple accused of tabcorp fraud</td>\n    </tr>\n    <tr>\n      <th>102484</th>\n      <td>doctors optimistic after heart surgery for east</td>\n      <td>doctsor optimistic after ehrat surgery for aets</td>\n    </tr>\n  </tbody>\n</table>\n<p>57208 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "correct_mine = 0\n",
    "correct_norvig = 0\n",
    "total = 0\n",
    "\n",
    "bar = tqdm(df.iloc[:1000].iterrows(), total=1000)\n",
    "for row in bar:\n",
    "    text_toks = row[1].text.split()\n",
    "    augmented_text_toks = row[1].augmented_text.split()\n",
    "    \n",
    "    # Correct using my implementation\n",
    "    fixed_mine = c.correct(row[1].augmented_text, verbose=0)\n",
    "    fixed_mine_toks = fixed_mine.split()\n",
    "    \n",
    "    # Correct using Norvig's implementation\n",
    "    norvig = [correction(tok) for tok in augmented_text_toks]\n",
    "    \n",
    "    # Calculate the words that should be corrected\n",
    "    for i in range(len(text_toks)):\n",
    "        if text_toks[i] != augmented_text_toks[i]:\n",
    "            total += 1\n",
    "            if fixed_mine_toks[i].lower() == text_toks[i]:\n",
    "                correct_mine += 1\n",
    "            if norvig[i] == text_toks[i]:\n",
    "                correct_norvig += 1\n",
    "    bar.set_postfix_str(f\"Accuracy mine: {correct_mine / total: 0.4f}, accuracy Norvig's: {correct_norvig / total: 0.4f}\")\n",
    "                \n",
    "print(f\"Accuracy of my solution: {correct_mine / total: 0.4f}\")\n",
    "print(f\"Accuracy of Norvig's solution: {correct_norvig / total: 0.4f}\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T03:09:34.998103Z",
     "iopub.execute_input": "2024-03-23T03:09:34.998546Z",
     "iopub.status.idle": "2024-03-23T03:09:35.008533Z",
     "shell.execute_reply.started": "2024-03-23T03:09:34.998510Z",
     "shell.execute_reply": "2024-03-23T03:09:35.006694Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [37:44<00:00,  2.26s/it, Accuracy mine:  0.6150, accuracy Norvig's:  0.4021]\nAccuracy of my solution:  0.6150\nAccuracy of Norvig's solution:  0.4021\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, my corrector scores much better results (even though the results are not perfect, probably, because of the dataset quality), so the implementation can be considered successful"
   ],
   "metadata": {}
  }
 ]
}
